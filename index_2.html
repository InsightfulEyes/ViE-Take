<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Real-world Affective Faces (RAF) Database</title>

<style type="text/css">
body,td,th {
    font-size: 16px;
}
body {
    border: 2px rgba(255,255,255,.38) solid;
    border-radius: 4px;
    margin-left: 100px;
    margin-right: 100px;
    margin-top: 18px;
}
pre {  
font-size: 12pt;  
background-color: #fffff4;  
border: 1px solid #999;
line-height: 20px; 
}
</style>
</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
    <h1><font size="7"><center>Real-world Affective Faces Database</center></font></h1>
    <font size="6" color="#0000CC"><center><a href="#dataset">RAF-DB</a></center></font>
  </div><br>

      <div class="show">
        <center><img src="image2.jpg" border="0" width="80%">
        </center>
      </div><br>

      <div class="section details">
      <a id="dataset"></a>
        <h2><font face="Tahoma">Details</font><br></h2>
    <p><b>Real-world Affective Faces Database (RAF-DB)</b> is a large-scale facial expression database with around <b>30K</b> great-diverse facial images downloaded from the Internet. Based on the crowdsourcing annotation, each image has been independently labeled by about <b>40</b> annotators. Images in this database  are of great variability in subjects' age, gender and ethnicity, head poses, lighting conditions, occlusions, (e.g. glasses, facial hair or self-occlusion), post-processing operations (e.g. various filters and special effects), etc. RAF-DB has large diversities, large quantities, and rich annotations, including:</p>

    <ul>
    <li><p><b>29672</b> number of <b>real-world images</b>,</p></li>
    <li>
      <p>a <b>7-dimensional expression distribution</b> vector for each image,</p></li>
    <li><p>two different subsets: <b>single-label subset</b>, including <b>7</b> classes of basic emotions; <b>two-tab subset</b>, including <b>12</b> classes of compound emotions,</p></li>
    <li>
      <p><b>5 accurate landmark locations</b>, <b>37 automatic landmark locations</b>, <b>bounding box</b>, <b>race, age range</b> and  <b>gender</b> <b> attributes</b> annotations per image,</p></li>
    <li><p><b>baseline</b> classifier outputs for basic emotions and compound emotions.</p></li>
    </ul>
    To be able to objectively measure the performance for
the followers' entries, the database has been split into a training set and a test set where the size of training set is five times larger than test set, and expressions in both sets have a near-identical distribution.
 </div><br>
    
      <div class="section overview">
      <h2><font face="Tahoma">Sample Images</font></h2><br>
    <center><img src="RAF-DB.png" border="0" width="80%">
</center>
</div><br>

 <div class="section contact">
       <h2><font face="Tahoma">Terms & Conditions</font></h2>
    <ul>
    <li><p>The RAF database is available for <b>non-commercial research purposes</b> only.</p></li>
    <li><p>All images of the RAF database are obtained from the Internet which are not property of PRIS, Beijing University of Posts and Telecommunications. The PRIS is not responsible for the content nor the meaning of these images.</p></li>
    <li><p>You agree <b>not to</b> reproduce, duplicate, copy, sell, trade, resell or exploit for any commercial purposes, any portion of the images and any portion of derived data.</p></li>
    <li><p>You agree <b>not to</b> further copy, publish or distribute any portion of the RAF database. Except, for internal use at a single site within the same organization it is allowed to make copies of the dataset.</p></li>
    <li><p>The PRIS reserves the right to terminate your access to the RAF database at any time.</p></li>
</ul>
    </div><br>
    
        <div class="password">
      <h2><font face="Tahoma"> How to get the Password</font> </h2><br>
      This database is publicly available. It is free for professors and researcher scientists affiliated to a University.<br/>
      Permission to use but not reproduce or distribute the RAF database is granted to all researchers given that the following steps are properly followed:
      <p>Send an e-mail to Shan Li <a href="mailto:queenie3@live.com">(email1)</a>  before downloading the database. You will need a password to access the files of this database. Your Email MUST be set from a valid University account and MUST include the following text:</p>
      


<pre>
<b>Subject</b>: Application to download the RAF Face Database          
<b>Name</b>: &lt;your first and last name&gt;
<b>Affiliation</b>: &lt;University where you work&gt;
<b>Department</b>: &lt;your department&gt;
<b>Position</b>: &lt;your job title&gt;
<b>Email</b>: &lt;must be the email at the above mentioned institution&gt;<br />
I have read and agree to the terms and conditions specified in the RAF face database webpage. 
This database will only be used for research purposes. 
I will not make any part of this database available to a third party. 
I'll not sell any part of this database or make any profit from its use.</pre>
      
      
        </div><br>

      <div class="section download">
      <h2><font face="Tahoma">Content Preview</font></h2>
      <ul>             
           <li class="grid">
          <div class="griditem">
<h3><b>Single-label Subset</b> (Basic emotions)</h3>
      
          <table>
            <tr>
          <td><center><img src="icon_zip.png" border="0" width="30%"></a></center></td>
          <td><center><img src="icon_zip.png" border="0" width="30%"></a></center></td>
           <td><center><img src="icon_txt.png" border="0" width="30%"></a></center></td>
          <td><center><img src="icon_txt.png" border="0" width="30%"></a></center></td>
          </tr>
          <tr>
          <td><center>
          Image
          </center></td>
          <td><center>Annotation</center></td>          
          <td><center>Emotion Label</center></td> 
          <td><center>READ ME</center></td>
          </tr>
        </table>
 </div>
 </li>
 </ul>                       
<br>

 <ul>             
           <li class="grid">
          <div class="griditem">
            <h3>Two-tab Subset (Compound emotions)</h3>
          <table>
          <tr>
          <td><center><img src="icon_zip.png" border="0" width="30%"></a></center></td>
          <td><center><img src="icon_zip.png" border="0" width="30%"></a></center></td>
          <td><center><img src="icon_txt.png" border="0" width="30%"></a></center></td>
          <td><center><img src="icon_txt.png" border="0" width="30%"></a></center></td>
          </tr>
          <tr>
          <td><center>
          Image
          </center></td>
          <td><center>Annotation</center></td>          
          <td><center>Emotion Label</center></td> 
          <td><center>READ ME</center></td>
          </tr>
          </table>
 </div>
 </li>
 </ul> 
 <p><br>
   
   
   For more details of the dataset, please refer to the paper "<a href="li_RAFDB_2017_CVPR.pdf" target="_blank">Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild</a>".
   <br>
   * Please note that the RAF database is partially public. And the other 10k images are neither basic nor compound emotions which will be released afterwards.
      </p>
      </div>
      <br>

      
    
          <div class="section code">
    <h2><font face="Tahoma">Code</font></h2>
    
    We have uploaded the configuration parameters of the DLP-CNN and the hyper-parameters of the trianing process. 
    <br>
    You can download it from <a href="caffe-expression.zip">here</a>.
    </div>
    <br>


    <div class="section citation">
    <h2><font face="Tahoma">Citation</font></h2>
    <div class="section bibtex">
      <pre>@inproceedings{li2017reliable,
title={Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild},
author={Li, Shan and Deng, Weihong and Du, JunPing},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages={2584--2593},
year={2017},
organization={IEEE}
}</pre>  

      <pre>@article{li2019reliable,
title={Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition},
author={Li, Shan and Deng, Weihong},
journal={IEEE Transactions on Image Processing},
volume={28},
number={1},
pages={356--370},
year={2019},
publisher={IEEE}
}</pre>  
    </div>
    </div>
    <br>
    
     <div class="section related work">
    <h2><font face="Tahoma">Related Work</font></h2>

    The following papers have employed RAF-DB for facial expression recognition.
    <br><br>
    <ul>
    <li>Zeng, Jiabei, Shiguang Shan, and Xilin Chen. "Facial Expression Recognition with Inconsistently Annotated Datasets." <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>. 2018.</li>
    <li>Vielzeuf, Valentin, et al. "An Occam's Razor View on Learning Audiovisual Emotion Recognition with Small Training Sets." <i>Proceedings of the 2018 on International Conference on Multimodal Interaction. ACM</i>, 2018.</li>
    <li>Khan, Ahmed Shehab, et al. "Group-Level Emotion Recognition using Deep Models with A Four-stream Hybrid Network." <i>Proceedings of the 2018 on International Conference on Multimodal Interaction. ACM</i>, 2018.</li>
    <li>Zhiwen Liu, Shan Li, and Weihong Deng. "Boosting-POOF: Boosting Part Based One vs One Feature for Facial Expression Recognition in the Wild", in <i>The 2nd International Workshop on Biometrics in the Wild (BWild),</i> 2017.</li>
    <li>Zhiwen Liu, Shan Li, and Weihong Deng. "Real-World Facial Expression Recognition Using Metric Learning Method", in <i>Chinese Conference on Biometric Recognition (CCBR),</i> 2016.</li>
     <li>Zhiwen Liu, Shan Li, and Weihong Deng. "Recognizing Compound Emotional Expression in Real-world using Metric Learning Method", in <i>Chinese Conference on Biometric Recognition (CCBR),</i> 2016.</li>
     <li>Shan Li and Weihong Deng. "Real world expression recognition: A highly imbalanced detection problem", in <i>9th IAPR International Conference on Biometrics (ICB)</i>, 2016</li>
     <li>... ...</li>
</ul>
</div><br>

     <div class="section contact">
    <h2><font face="Tahoma">Contact</font></h2>
    
    Please contact <a href="mailto:queenie3@live.com">Shan Li</a> and <a href="mailto:whdeng@bupt.edu.cn">Weihong Deng</a> for questions about the database.
    </div><br>
</div></div>
</body></html>
